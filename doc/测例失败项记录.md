# 已知及偶然性的测例失败项待办列表

- [ ] ipc 之后执行 proc 测例概率因 page fault 终止

该现象共出现过两次，分别位于 exit 与 fork 测例，其中 fork 测例中产生的 page fault 位于 rq_insert 的空指针成员偏移访问，对应变量未知。

重复五六次尝试复现，复现无果，故在此记录。

- [ ] ls 操作概率导致崩溃重启

原始描述：无

最新描述：修复 vfs 问题时发现 vfs 较为脆弱，几乎每一次因漏洞而失败的 fs 操作都会导致在这之后的 fs 操作失败，包括但不限于崩溃重启、并发读写原子性测试失败、page fault 等等。经过相关修复后该项错误可能一并解决，已长期未触发该现象，但暂时无法确定。

- [ ] 执行 fs 测例后执行 proc 测例概率导致崩溃重启

该现象首次出现的场景为：疯狂执行 fs 测例并全数通过，在这之后紧接着执行 proc 测例系统崩溃重启后，在这之后继续执行 proc 测例依旧崩溃重启，在这之后等待若干秒后再执行 proc 测例运行正常。

重新仿真模拟上述过程，未能复现。

情景补充：在 v2.3.0.beta.1 版本下启动仿真常规速度先执行 fs 测例再执行 proc，出现崩溃重启现象。

- [ ] 非实时调度队列结构失效导致断言失败

具体指代 kernel/sched.c:is_rq_empty 中的关于队列判空的断言，该错误在多个高并发场景下均曾出现。

- [x] 内存过大时系统启动失败

当内存大小大于等于 1004.5 MiB 时，AHCI Controller 异常，报错 `error: ahci mode not supported`，于 bootloader 加载内核失败导致系统启动失败。

其中 1004 MiB 启动成功，为最大可启动内存。

调试结果：ahci controller 的 mmio 在本实现中等值映射在 0xfebf1000，当达到 1005 MiB 后该线性地址将提前被映射在内核空间，导致 mmio 的映射失败，随后对设备的访问实际落在 ram 上而非设备内存空间。

解决方案：

**方案一**

在 1MiB\~3GiB 之间随便找个线性地址映射到设备地址。

**方案二**

若已经映射，替换原页表项完成设备配置，在内核加载完成后跳入内核前回滚该操作。

注意事项：kernel 里完成的 ahci 设备初始化也是一样，记得要改。

最终解决方案：

1. loader 采取添加 vma alloc 通过分配空闲线性地址处理
2. kernel 由于在共享页表搜索空闲实现，无需更改，但对于大内存（高于 1G）需额外限制 buddy 分配到内核态的物理内存
3. 引入 ioremap/iounmap 实现一般的设备 mmio

- [ ] dentry 死锁 / hd\_service 死锁

反复运行 proc 测例约一个半小时后，Execve.WaitAfterExecve 测试项的 33、36 和 38 号进程在 /bin/execve 的 bin 目录项上死锁，其中 33、36 号进程在 kern\_execve 下打开文件下层的 get\_dentry 处死锁，同时 38 号进程位于 hd\_rdwt\_sched 的 wait\_event(&hdque)，获得锁者为 38 号进程。

其中 38 号进程在 kern\_execve /bin/execve 下的  vfs\_put\_dentry bin 下的 vfs\_sync\_inode 下直到 wait\_event(&hdque)，该进程在 wait\_event(&hdque) 主动 sched\_yield 前被调度走且始终未被唤醒。对应的，此时 hd\_service 进程正在 ksem\_wait(&hdque_full, 1)。

备注：该问题暂时出现次数 2，最后一次出现时机在提交 e28c6e4。

- [ ] slab 分配的对象无法释放

反复随机运行测例，概率在 ipc 测例 的 MsgQueueSend.WaitUntil 测试项中的子进程 msgctl 中的 pop_front_msg_node 中触发。

具体地，pop_front_msg_node 中的 phy\_kfree 从 slab 中尝试释放对象，在 slab\_free 中的查找对象循环中始终无法在 cache 的 partial/full 的 slab 链中找到对应对象，继而无限重复尝试失败。

在这过程中测例仅父子进程，且父进程处于 wait 睡眠状态等待子进程结束。

情景补充：在以上情景的复现中再次调试，发现待释放对象 0xc4f70040 位于 cache 的 empty 链的第 2 个 slab 0xc4f70000 中，对应 slab 位图标记和总对象数均为 0，结构未损坏，可猜测该地址被二次释放。

警告：若以上提及的二次释放成立，则原使用处可能使用了已经释放的内存，这将导致不可预期的错误。

- [x] fs 相关操作的耗时随运行时长增加而显著增加

反复随机运行测例一小时及以上，fs 测例运行耗时从百毫秒级暴涨至十秒级，对于约一小时处的运行结果，耗时翻了将近七十倍。

严格来说这并非测例 NG，但是考虑到问题过于严重，不得不将其视为需要修复/优化的紧急待办项。

解决方案：不需要解决，出现该问题的原因是原测例在 /tmp 创建文件但是并不删除，越来越慢纯属是 tmp 文件太多了，在 fat32 impl 中找文件找空位啥的操作耗时变长。在 shell 每次执行完测例里手动清 /tmp 再跑一小时随机测例，得到 fs 的运行时长始终稳定在 \~180ms，可以得到验证。

- [x] 增加 ACPI SDST 设备树遍历枚举后系统异常崩溃

扫描动作产生于 kernel\_main 阶段，使用的栈为 kernel.asm 中 .bss 段中给出的 4KB 空间。

递归遍历导致栈指针下溢覆写了相邻的 irq handler 表，导致在覆写之后的时钟中断中跳转到了无效的 handler 从而引发 page fault 崩溃。

解决方案：kernel\_main 阶段切换栈区，一方面调整栈的容量至 1MB，另一方面在栈的上下边界建立守卫页以便在类似异常发生时快速定位。

- [x] 真机 acpi 初始化在 2K 内存分配的内存块页面状态异常导致释放处断言失败

我得说这并不偶然而极其确定，你甚至可以百分百复现这个问题，仿真能过大概是运气好没什么地方用到了 2K 分配而 PASS 了，事实上这个问题要远比描述的大得多。

目前的 slab 分配是由单个物理页建立 slab，该物理页同时容纳 slab 结构与关联的对象池。

在之前的某个版本中，李荣处理了 slab 对象池未对齐而导致的异常，但是当前的错误显然表明的当时处理的不完善。

该错误的概括表述为“对象池及相关状态的不一致性”，相关描述如下：

1. 分配一页作为 slab 页，页首初始化 slab 结构，其后空间依据对象大小幂二对齐取为对象池基址
2. 因对齐而损失的空间在 slab 方面记录为保留的已分配对象，影响位图状态，已分配对象个数，对象索引
3. 因对齐而损失的空间在 cache 方面记录为原始便不用于对象分配的空间，影响最大对象数，slab 的对象池基址
4. 分配并取对象地址的方法为对象池基址偏移对象索引与对象大小之积

附加陈述有下：

1. 从 cache 分配而所有 slab 持有的空闲对象不足时创建新 slab，该新 slab 考虑对齐问题初始化已分配对象个数与位图状态，并置入 PARTIAL 链
2. slab 的移动操作发生在任意对象的分配/释放之后

引发的错误有下：

1. 因空间损失的存在，在 2、3 不一致性的条件下，任意移入 EMPTY 链的 slab 均为伪 EMPTY，其位图状态与已分配对象个数的属性处于损坏状态
2. 因 2、3 不一致性的存在，任意移入 FULL 链的 slab 均为伪 FULL，其仍剩余 1 空闲对象可分配
3. 在 2、3 不一致性的前提下，对象索引在 4 的计算方式下造成分配出的对象地址的上溢（此处发现的问题）

其中，由于附加描述的存在，错误 3 的产生具有必要条件“分配大小为 2K”。此时尽管 slab 的已分配对象个数已达到最大对象数，但其仍位于 buddy 的 PARTIAL 链，故将始终执行为错误 3 的路径；而对于 2K 以下的对象分配，首次分配为“有效”分配，同时附加描述 2 使得对象分配达到伪上限移入 FULL 链，此时执行为错误路径 2，但另一方面，该错误路径也使得其免于对象地址上溢，从而表现出“只有 2K 分配会报错，而其他小内存分配运行非常良好”的假象。

解决方案：取当前 slab 的状态维护为主，将因对齐而损失的空间记录为保留的已分配对象，同时对象池基址还原为页面基址以确保一致性。

- [ ] 随机测例运行 2.5hr 后 fs read 测例因 hd_service 断言失败而失败

基准代码 a93db95，错误日志如下：

```plain
kernel/hd.c:hd_rdwt:262: fatal: assertion failed: nr_sector < part->size
backtrace of process [pid=2]:
    => 0xbfffefcc (0xc0603474)
    => 0xbfffeffc (0)
[#GP General Protection] eip=0xc06062a0 eflags=0x1013 cs=0x5 err_code=0 from "hd_service" [pid=2]
2025-03-16 00:27:58 [#PF Page Fault] eip=0xc060c66e eflags=0x1282 cs=0x8 err_code=0 from "hd_service" [pid=2]
```

表现为因磁盘读写越界分区大小引发的连锁错误。
